{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neetushibu/IontheFold-Team6/blob/main/Enhanced_Charge_Analyzer_Resume_FAST_SAFE_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYcyZ86nriz8",
        "outputId": "2cda653b-e18a-40af-cada-8b99dd3fa287"
      },
      "id": "NYcyZ86nriz8",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Tuple, Dict\n",
        "from scipy.spatial.distance import cdist\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import glob\n",
        "import json\n",
        "import csv\n",
        "import threading\n",
        "import time\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "OJPkJ5hwq-BS"
      },
      "id": "OJPkJ5hwq-BS",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedChargeAffinityAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.atoms = []\n",
        "        self.bonds = []\n",
        "        self.filename = \"\"\n",
        "        self.charged_residues = []\n",
        "        self.binding_sites = []\n",
        "        self.sequence_data = {}\n",
        "\n",
        "        # Real-time file writing setup\n",
        "        self.output_writers = {}\n",
        "        self.output_files = {}\n",
        "        self.write_lock = threading.Lock()\n",
        "\n",
        "    def open_file(self, filepath: str) -> str:\n",
        "        \"\"\"Open and read a file, handling both compressed and uncompressed formats.\"\"\"\n",
        "        self.filename = os.path.basename(filepath)\n",
        "\n",
        "        try:\n",
        "            if filepath.endswith('.gz'):\n",
        "                with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
        "                    content = f.read()\n",
        "                print(f\"✓ Successfully opened compressed file: {self.filename}\")\n",
        "            else:\n",
        "                with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                    content = f.read()\n",
        "                print(f\"✓ Successfully opened file: {self.filename}\")\n",
        "\n",
        "            return content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error opening file {filepath}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def extract_sequence_from_pdb(self, content: str) -> Dict:\n",
        "        \"\"\"Extract protein sequences from PDB content.\"\"\"\n",
        "        lines = content.split('\\n')\n",
        "        sequences = {}\n",
        "        current_chain = None\n",
        "\n",
        "        # Standard amino acid mapping\n",
        "        aa_mapping = {\n",
        "            'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',\n",
        "            'GLU': 'E', 'GLN': 'Q', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
        "            'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',\n",
        "            'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
        "        }\n",
        "\n",
        "        # First try to get sequence from SEQRES records (preferred)\n",
        "        for line in lines:\n",
        "            if line.startswith('SEQRES'):\n",
        "                chain_id = line[11].strip()\n",
        "                if chain_id not in sequences:\n",
        "                    sequences[chain_id] = {'seqres': '', 'atom_based': '', 'length': 0}\n",
        "\n",
        "                # Extract residues from SEQRES line\n",
        "                residues = line[19:].strip().split()\n",
        "                for res in residues:\n",
        "                    if res in aa_mapping:\n",
        "                        sequences[chain_id]['seqres'] += aa_mapping[res]\n",
        "                    else:\n",
        "                        sequences[chain_id]['seqres'] += 'X'  # Unknown residue\n",
        "\n",
        "        # Also extract sequence from ATOM records as backup\n",
        "        chain_residues = {}\n",
        "        for line in lines:\n",
        "            if line.startswith('ATOM'):\n",
        "                try:\n",
        "                    chain = line[21].strip()\n",
        "                    residue_name = line[17:20].strip()\n",
        "                    residue_id = int(line[22:26].strip()) if line[22:26].strip() else 0\n",
        "\n",
        "                    if chain not in chain_residues:\n",
        "                        chain_residues[chain] = {}\n",
        "\n",
        "                    if residue_id not in chain_residues[chain]:\n",
        "                        chain_residues[chain][residue_id] = residue_name\n",
        "\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "\n",
        "        # Build atom-based sequences\n",
        "        for chain_id, residues in chain_residues.items():\n",
        "            if chain_id not in sequences:\n",
        "                sequences[chain_id] = {'seqres': '', 'atom_based': '', 'length': 0}\n",
        "\n",
        "            sorted_residues = sorted(residues.items())\n",
        "            atom_seq = ''\n",
        "            for res_id, res_name in sorted_residues:\n",
        "                if res_name in aa_mapping:\n",
        "                    atom_seq += aa_mapping[res_name]\n",
        "                else:\n",
        "                    atom_seq += 'X'\n",
        "            sequences[chain_id]['atom_based'] = atom_seq\n",
        "\n",
        "        # Finalize sequences (prefer SEQRES, fallback to atom-based)\n",
        "        for chain_id in sequences:\n",
        "            final_seq = sequences[chain_id]['seqres'] or sequences[chain_id]['atom_based']\n",
        "            sequences[chain_id]['final_sequence'] = final_seq\n",
        "            sequences[chain_id]['length'] = len(final_seq)\n",
        "\n",
        "        self.sequence_data = sequences\n",
        "        print(f\"✓ Extracted sequences for {len(sequences)} chains\")\n",
        "        for chain_id, data in sequences.items():\n",
        "            print(f\"  Chain {chain_id}: {data['length']} residues\")\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    def parse_pdb(self, content: str) -> None:\n",
        "        \"\"\"Parse PDB format file content and extract sequences.\"\"\"\n",
        "        self.atoms = []\n",
        "\n",
        "        # Extract sequences first\n",
        "        self.extract_sequence_from_pdb(content)\n",
        "\n",
        "        # Parse atoms\n",
        "        lines = content.split('\\n')\n",
        "        for line in lines:\n",
        "            if line.startswith('ATOM') or line.startswith('HETATM'):\n",
        "                try:\n",
        "                    atom_id = int(line[6:11].strip())\n",
        "                    atom_name = line[12:16].strip()\n",
        "                    residue_name = line[17:20].strip()\n",
        "                    chain = line[21].strip()\n",
        "                    residue_id = int(line[22:26].strip()) if line[22:26].strip() else 0\n",
        "                    x = float(line[30:38].strip())\n",
        "                    y = float(line[38:46].strip())\n",
        "                    z = float(line[46:54].strip())\n",
        "                    element = line[76:78].strip() or atom_name[0]\n",
        "\n",
        "                    self.atoms.append({\n",
        "                        'id': atom_id,\n",
        "                        'name': atom_name,\n",
        "                        'element': element,\n",
        "                        'residue': residue_name,\n",
        "                        'chain': chain,\n",
        "                        'residue_id': residue_id,\n",
        "                        'x': x, 'y': y, 'z': z\n",
        "                    })\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "\n",
        "        print(f\"✓ Parsed {len(self.atoms)} atoms from PDB file\")\n",
        "\n",
        "    def setup_realtime_writers(self, output_folder: str, timestamp: str) -> None:\n",
        "        \"\"\"Setup CSV writers for real-time data capture.\"\"\"\n",
        "\n",
        "        # Setup file paths\n",
        "        files_config = {\n",
        "            'summary': {\n",
        "                'path': os.path.join(output_folder, f\"charge_analysis_summary_{timestamp}.csv\"),\n",
        "                'fieldnames': [\n",
        "                    'protein_id', 'filename', 'analysis_timestamp', 'total_atoms', 'total_residues',\n",
        "                    'total_chains', 'sequence_length', 'longest_chain_sequence',\n",
        "                    'total_charged_residues', 'positive_residues', 'negative_residues',\n",
        "                    'net_protein_charge', 'total_molecular_charge',\n",
        "                    'total_binding_sites', 'strong_binding_sites', 'moderate_binding_sites',\n",
        "                    'best_binding_potential', 'total_interactions', 'strong_interactions',\n",
        "                    'arg_count', 'lys_count', 'his_count', 'asp_count', 'glu_count',\n",
        "                    'chain_sequences'  # JSON string of all chain sequences\n",
        "                ]\n",
        "            },\n",
        "            'sequences': {\n",
        "                'path': os.path.join(output_folder, f\"protein_sequences_{timestamp}.csv\"),\n",
        "                'fieldnames': [\n",
        "                    'protein_id', 'chain_id', 'sequence_length', 'sequence',\n",
        "                    'seqres_sequence', 'atom_based_sequence', 'analysis_timestamp'\n",
        "                ]\n",
        "            },\n",
        "            'binding_sites': {\n",
        "                'path': os.path.join(output_folder, f\"binding_sites_{timestamp}.csv\"),\n",
        "                'fieldnames': [\n",
        "                    'protein_id', 'site_rank', 'x_coord', 'y_coord', 'z_coord',\n",
        "                    'potential', 'binding_type', 'site_size', 'analysis_timestamp'\n",
        "                ]\n",
        "            },\n",
        "            'interactions': {\n",
        "                'path': os.path.join(output_folder, f\"charge_interactions_{timestamp}.csv\"),\n",
        "                'fieldnames': [\n",
        "                    'protein_id', 'residue1', 'residue2', 'distance', 'energy',\n",
        "                    'interaction_type', 'strength', 'analysis_timestamp'\n",
        "                ]\n",
        "            },\n",
        "            'progress': {\n",
        "                'path': os.path.join(output_folder, f\"analysis_progress_{timestamp}.csv\"),\n",
        "                'fieldnames': [\n",
        "                    'protein_id', 'filename', 'status', 'timestamp', 'error_message',\n",
        "                    'processing_time_seconds'\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Open files and create writers\n",
        "        for file_type, config in files_config.items():\n",
        "            try:\n",
        "                file_handle = open(config['path'], 'w', newline='', encoding='utf-8')\n",
        "                writer = csv.DictWriter(file_handle, fieldnames=config['fieldnames'])\n",
        "                writer.writeheader()\n",
        "                file_handle.flush()  # Ensure header is written immediately\n",
        "\n",
        "                self.output_files[file_type] = file_handle\n",
        "                self.output_writers[file_type] = writer\n",
        "\n",
        "                print(f\"✓ Setup real-time writer: {config['path']}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Failed to setup writer for {file_type}: {e}\")\n",
        "\n",
        "    def setup_realtime_writers_resume(self, output_folder: str, timestamp: str, append: bool = False) -> None:\n",
        "        \"\"\"Setup CSV writers for real-time data capture with resume capability.\"\"\"\n",
        "\n",
        "        # Setup file paths (same as before)\n",
        "        files_config = {\n",
        "            'summary': {\n",
        "                'path': os.path.join(output_folder, f\"charge_analysis_summary_{timestamp}.csv\"),\n",
        "                'fieldnames': [\n",
        "                    'protein_id', 'filename', 'analysis_timestamp', 'total_atoms', 'total_residues',\n",
        "                    'total_chains', 'sequence_length', 'longest_chain_sequence',\n",
        "                    'total_charged_residues', 'positive_residues', 'negative_residues',\n",
        "                    'net_protein_charge', 'total_molecular_charge',\n",
        "                    'total_binding_sites', 'strong_binding_sites', 'moderate_binding_sites',\n",
        "                    'best_binding_potential', 'total_interactions', 'strong_interactions',\n",
        "                    'arg_count', 'lys_count', 'his_count', 'asp_count', 'glu_count',\n",
        "                    'chain_sequences'\n",
        "                ]\n",
        "            },\n",
        "            'sequences': {\n",
        "                'path': os.path.join(output_folder, f\"protein_sequences_{timestamp}.csv\"),\n",
        "                'fieldnames': [\n",
        "                    'protein_id', 'chain_id', 'sequence_length', 'sequence',\n",
        "                    'seqres_sequence', 'atom_based_sequence', 'analysis_timestamp'\n",
        "                ]\n",
        "            },\n",
        "            'binding_sites': {\n",
        "                'path': os.path.join(output_folder, f\"binding_sites_{timestamp}.csv\"),\n",
        "                'fieldnames': [\n",
        "                    'protein_id', 'site_rank', 'x_coord', 'y_coord', 'z_coord',\n",
        "                    'potential', 'binding_type', 'site_size', 'analysis_timestamp'\n",
        "                ]\n",
        "            },\n",
        "            'interactions': {\n",
        "                'path': os.path.join(output_folder, f\"charge_interactions_{timestamp}.csv\"),\n",
        "                'fieldnames': [\n",
        "                    'protein_id', 'residue1', 'residue2', 'distance', 'energy',\n",
        "                    'interaction_type', 'strength', 'analysis_timestamp'\n",
        "                ]\n",
        "            },\n",
        "            'progress': {\n",
        "                'path': os.path.join(output_folder, f\"analysis_progress_{timestamp}.csv\"),\n",
        "                'fieldnames': [\n",
        "                    'protein_id', 'filename', 'status', 'timestamp', 'error_message',\n",
        "                    'processing_time_seconds'\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Open files and create writers\n",
        "        for file_type, config in files_config.items():\n",
        "            try:\n",
        "                # Check if file exists for append mode\n",
        "                file_exists = os.path.exists(config['path'])\n",
        "                mode = 'a' if (append and file_exists) else 'w'\n",
        "\n",
        "                file_handle = open(config['path'], mode, newline='', encoding='utf-8')\n",
        "                writer = csv.DictWriter(file_handle, fieldnames=config['fieldnames'])\n",
        "\n",
        "                # Only write header if creating new file\n",
        "                if mode == 'w':\n",
        "                    writer.writeheader()\n",
        "\n",
        "                file_handle.flush()\n",
        "\n",
        "                self.output_files[file_type] = file_handle\n",
        "                self.output_writers[file_type] = writer\n",
        "\n",
        "                status = \"appending to\" if mode == 'a' else \"created\"\n",
        "                print(f\"✓ Setup real-time writer ({status}): {config['path']}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Failed to setup writer for {file_type}: {e}\")\n",
        "\n",
        "    def write_progress_update(self, protein_id: str, filename: str, status: str,\n",
        "                             error_message: str = \"\", processing_time: float = 0.0) -> None:\n",
        "        \"\"\"Write progress update in real-time.\"\"\"\n",
        "        if 'progress' not in self.output_writers:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            with self.write_lock:\n",
        "                row = {\n",
        "                    'protein_id': protein_id,\n",
        "                    'filename': filename,\n",
        "                    'status': status,\n",
        "                    'timestamp': datetime.now().isoformat(),\n",
        "                    'error_message': error_message,\n",
        "                    'processing_time_seconds': round(processing_time, 2)\n",
        "                }\n",
        "                self.output_writers['progress'].writerow(row)\n",
        "                self.output_files['progress'].flush()\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Failed to write progress update: {e}\")\n",
        "\n",
        "    def write_sequence_data(self, protein_id: str) -> None:\n",
        "        \"\"\"Write sequence data in real-time.\"\"\"\n",
        "        if 'sequences' not in self.output_writers or not self.sequence_data:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            with self.write_lock:\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                for chain_id, seq_data in self.sequence_data.items():\n",
        "                    row = {\n",
        "                        'protein_id': protein_id,\n",
        "                        'chain_id': chain_id,\n",
        "                        'sequence_length': seq_data['length'],\n",
        "                        'sequence': seq_data['final_sequence'],\n",
        "                        'seqres_sequence': seq_data['seqres'],\n",
        "                        'atom_based_sequence': seq_data['atom_based'],\n",
        "                        'analysis_timestamp': timestamp\n",
        "                    }\n",
        "                    self.output_writers['sequences'].writerow(row)\n",
        "                self.output_files['sequences'].flush()\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Failed to write sequence data: {e}\")\n",
        "\n",
        "    def write_binding_sites_data(self, protein_id: str) -> None:\n",
        "        \"\"\"Write binding sites data in real-time.\"\"\"\n",
        "        if 'binding_sites' not in self.output_writers or not self.binding_sites:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            with self.write_lock:\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                # Sort binding sites by potential (best first)\n",
        "                sorted_sites = sorted(self.binding_sites, key=lambda x: x['potential'])\n",
        "\n",
        "                for i, site in enumerate(sorted_sites, 1):\n",
        "                    row = {\n",
        "                        'protein_id': protein_id,\n",
        "                        'site_rank': i,\n",
        "                        'x_coord': round(site['position'][0], 2),\n",
        "                        'y_coord': round(site['position'][1], 2),\n",
        "                        'z_coord': round(site['position'][2], 2),\n",
        "                        'potential': round(site['potential'], 2),\n",
        "                        'binding_type': site.get('type', 'unknown'),\n",
        "                        'site_size': site.get('size', 1),\n",
        "                        'analysis_timestamp': timestamp\n",
        "                    }\n",
        "                    self.output_writers['binding_sites'].writerow(row)\n",
        "                self.output_files['binding_sites'].flush()\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Failed to write binding sites data: {e}\")\n",
        "\n",
        "    def write_interactions_data(self, protein_id: str, interactions: Dict) -> None:\n",
        "        \"\"\"Write interactions data in real-time.\"\"\"\n",
        "        if 'interactions' not in self.output_writers:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            with self.write_lock:\n",
        "                timestamp = datetime.now().isoformat()\n",
        "                for interaction in interactions.get('interactions', []):\n",
        "                    row = {\n",
        "                        'protein_id': protein_id,\n",
        "                        'residue1': interaction['residue1'],\n",
        "                        'residue2': interaction['residue2'],\n",
        "                        'distance': round(interaction['distance'], 2),\n",
        "                        'energy': round(interaction['energy'], 4),\n",
        "                        'interaction_type': interaction['type'],\n",
        "                        'strength': interaction['strength'],\n",
        "                        'analysis_timestamp': timestamp\n",
        "                    }\n",
        "                    self.output_writers['interactions'].writerow(row)\n",
        "                self.output_files['interactions'].flush()\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Failed to write interactions data: {e}\")\n",
        "\n",
        "    def write_summary_data(self, protein_summary: Dict) -> None:\n",
        "        \"\"\"Write summary data in real-time.\"\"\"\n",
        "        if 'summary' not in self.output_writers:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            with self.write_lock:\n",
        "                # Prepare sequence data for summary\n",
        "                chain_sequences = {}\n",
        "                total_sequence_length = 0\n",
        "                longest_sequence = \"\"\n",
        "\n",
        "                for chain_id, seq_data in self.sequence_data.items():\n",
        "                    chain_sequences[chain_id] = seq_data['final_sequence']\n",
        "                    total_sequence_length += seq_data['length']\n",
        "                    if len(seq_data['final_sequence']) > len(longest_sequence):\n",
        "                        longest_sequence = seq_data['final_sequence']\n",
        "\n",
        "                row = {\n",
        "                    'protein_id': protein_summary['protein_id'],\n",
        "                    'filename': protein_summary['filename'],\n",
        "                    'analysis_timestamp': protein_summary['analysis_timestamp'],\n",
        "                    'total_atoms': protein_summary['total_atoms'],\n",
        "                    'total_residues': protein_summary['total_residues'],\n",
        "                    'total_chains': protein_summary['total_chains'],\n",
        "                    'sequence_length': total_sequence_length,\n",
        "                    'longest_chain_sequence': longest_sequence,\n",
        "                    'total_charged_residues': protein_summary['total_charged_residues'],\n",
        "                    'positive_residues': protein_summary['positive_residues'],\n",
        "                    'negative_residues': protein_summary['negative_residues'],\n",
        "                    'net_protein_charge': round(protein_summary['net_protein_charge'], 2),\n",
        "                    'total_molecular_charge': round(protein_summary['total_molecular_charge'], 2),\n",
        "                    'total_binding_sites': protein_summary['total_binding_sites'],\n",
        "                    'strong_binding_sites': protein_summary['strong_binding_sites'],\n",
        "                    'moderate_binding_sites': protein_summary['moderate_binding_sites'],\n",
        "                    'best_binding_potential': round(protein_summary['best_binding_potential'], 2),\n",
        "                    'total_interactions': protein_summary['total_interactions'],\n",
        "                    'strong_interactions': protein_summary['strong_interactions'],\n",
        "                    'arg_count': protein_summary['positive_residue_types'].get('ARG', 0),\n",
        "                    'lys_count': protein_summary['positive_residue_types'].get('LYS', 0),\n",
        "                    'his_count': protein_summary['positive_residue_types'].get('HIS', 0),\n",
        "                    'asp_count': protein_summary['negative_residue_types'].get('ASP', 0),\n",
        "                    'glu_count': protein_summary['negative_residue_types'].get('GLU', 0),\n",
        "                    'chain_sequences': json.dumps(chain_sequences)\n",
        "                }\n",
        "                self.output_writers['summary'].writerow(row)\n",
        "                self.output_files['summary'].flush()\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Failed to write summary data: {e}\")\n",
        "\n",
        "    def close_realtime_writers(self) -> None:\n",
        "        \"\"\"Close all real-time writers.\"\"\"\n",
        "        with self.write_lock:\n",
        "            for file_type, file_handle in self.output_files.items():\n",
        "                try:\n",
        "                    file_handle.close()\n",
        "                    print(f\"✓ Closed writer: {file_type}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error closing {file_type}: {e}\")\n",
        "\n",
        "            self.output_files.clear()\n",
        "            self.output_writers.clear()\n",
        "\n",
        "    def assign_partial_charges(self) -> None:\n",
        "        \"\"\"Assign partial charges to atoms based on residue type and atom name.\"\"\"\n",
        "\n",
        "        # Standard amino acid partial charges (AMBER ff14SB-based)\n",
        "        charge_library = {\n",
        "            # Charged residues\n",
        "            'ARG': {'N': -0.47, 'CA': 0.26, 'C': 0.73, 'O': -0.59, 'CB': -0.00, 'CG': 0.04, 'CD': 0.05, 'NE': -0.52, 'CZ': 0.80, 'NH1': -0.86, 'NH2': -0.86},\n",
        "            'LYS': {'N': -0.40, 'CA': 0.25, 'C': 0.73, 'O': -0.59, 'CB': -0.01, 'CG': 0.01, 'CD': -0.04, 'CE': -0.01, 'NZ': -0.38},\n",
        "            'ASP': {'N': -0.42, 'CA': 0.27, 'C': 0.73, 'O': -0.59, 'CB': -0.16, 'CG': 0.81, 'OD1': -0.76, 'OD2': -0.76},\n",
        "            'GLU': {'N': -0.42, 'CA': 0.27, 'C': 0.73, 'O': -0.59, 'CB': -0.01, 'CG': 0.01, 'CD': 0.81, 'OE1': -0.76, 'OE2': -0.76},\n",
        "            'HIS': {'N': -0.42, 'CA': 0.27, 'C': 0.73, 'O': -0.59, 'CB': -0.09, 'CG': -0.05, 'ND1': -0.61, 'CD2': 0.13, 'CE1': 0.44, 'NE2': -0.61},\n",
        "\n",
        "            # Polar residues\n",
        "            'SER': {'N': -0.42, 'CA': 0.27, 'C': 0.73, 'O': -0.59, 'CB': 0.21, 'OG': -0.65},\n",
        "            'THR': {'N': -0.42, 'CA': 0.27, 'C': 0.73, 'O': -0.59, 'CB': 0.20, 'OG1': -0.68, 'CG2': -0.24},\n",
        "            'TYR': {'N': -0.42, 'CA': 0.27, 'C': 0.73, 'O': -0.59, 'CB': -0.01, 'CG': -0.19, 'CD1': -0.17, 'CD2': -0.17, 'CE1': -0.17, 'CE2': -0.17, 'CZ': 0.17, 'OH': -0.54},\n",
        "            'CYS': {'N': -0.42, 'CA': 0.27, 'C': 0.73, 'O': -0.59, 'CB': -0.11, 'SG': -0.31},\n",
        "            'ASN': {'N': -0.42, 'CA': 0.27, 'C': 0.73, 'O': -0.59, 'CB': -0.20, 'CG': 0.71, 'OD1': -0.59, 'ND2': -0.92},\n",
        "            'GLN': {'N': -0.42, 'CA': 0.27, 'C': 0.73, 'O': -0.59, 'CB': -0.01, 'CG': 0.01, 'CD': 0.71, 'OE1': -0.59, 'NE2': -0.92},\n",
        "\n",
        "            # Default for other residues (approximate)\n",
        "            'DEFAULT': {'N': -0.40, 'CA': 0.25, 'C': 0.70, 'O': -0.60, 'CB': -0.05}\n",
        "        }\n",
        "\n",
        "        # Assign charges\n",
        "        for atom in self.atoms:\n",
        "            residue = atom['residue']\n",
        "            atom_name = atom['name']\n",
        "\n",
        "            if residue in charge_library:\n",
        "                atom['charge'] = charge_library[residue].get(atom_name, 0.0)\n",
        "            else:\n",
        "                atom['charge'] = charge_library['DEFAULT'].get(atom_name, 0.0)\n",
        "\n",
        "            # Special handling for metals and ions\n",
        "            if atom['element'] in ['MG', 'CA', 'ZN', 'FE', 'MN']:\n",
        "                atom['charge'] = 2.0 if atom['element'] in ['MG', 'CA', 'ZN'] else 3.0\n",
        "            elif atom['element'] in ['CL']:\n",
        "                atom['charge'] = -1.0\n",
        "            elif atom['element'] in ['NA']:\n",
        "                atom['charge'] = 1.0\n",
        "\n",
        "        print(f\"✓ Assigned partial charges to {len(self.atoms)} atoms\")\n",
        "\n",
        "    def identify_charged_residues(self) -> None:\n",
        "        \"\"\"Identify and analyze charged residues.\"\"\"\n",
        "        charged_types = ['ARG', 'LYS', 'ASP', 'GLU', 'HIS']\n",
        "        self.charged_residues = []\n",
        "\n",
        "        residue_groups = defaultdict(list)\n",
        "        for atom in self.atoms:\n",
        "            if atom['residue'] in charged_types:\n",
        "                key = (atom['chain'], atom['residue'], atom['residue_id'])\n",
        "                residue_groups[key].append(atom)\n",
        "\n",
        "        for (chain, residue_type, residue_id), atoms in residue_groups.items():\n",
        "            # Calculate center of mass\n",
        "            coords = np.array([[a['x'], a['y'], a['z']] for a in atoms])\n",
        "            center = np.mean(coords, axis=0)\n",
        "\n",
        "            # Calculate net charge\n",
        "            net_charge = sum(atom.get('charge', 0.0) for atom in atoms)\n",
        "\n",
        "            # Determine charge type\n",
        "            if residue_type in ['ARG', 'LYS']:\n",
        "                charge_type = 'positive'\n",
        "            elif residue_type in ['ASP', 'GLU']:\n",
        "                charge_type = 'negative'\n",
        "            else:  # HIS\n",
        "                charge_type = 'variable'\n",
        "\n",
        "            self.charged_residues.append({\n",
        "                'chain': chain,\n",
        "                'residue': residue_type,\n",
        "                'residue_id': residue_id,\n",
        "                'center': center,\n",
        "                'atoms': atoms,\n",
        "                'net_charge': net_charge,\n",
        "                'charge_type': charge_type\n",
        "            })\n",
        "\n",
        "        print(f\"✓ Identified {len(self.charged_residues)} charged residues\")\n",
        "\n",
        "    def find_binding_pockets(self, grid_spacing: float = 2.0, pocket_threshold: float = -5.0) -> None:\n",
        "        \"\"\"Find potential binding pockets based on electrostatic potential.\"\"\"\n",
        "        if not hasattr(self.atoms[0], 'charge') or 'charge' not in self.atoms[0]:\n",
        "            print(\"⚠️  No charges assigned. Running charge assignment first...\")\n",
        "            self.assign_partial_charges()\n",
        "\n",
        "        # Create a grid around the protein\n",
        "        coords = np.array([[atom['x'], atom['y'], atom['z']] for atom in self.atoms])\n",
        "        min_coords = np.min(coords, axis=0) - 5.0\n",
        "        max_coords = np.max(coords, axis=0) + 5.0\n",
        "\n",
        "        # Sample grid points\n",
        "        x_range = np.arange(min_coords[0], max_coords[0], grid_spacing)\n",
        "        y_range = np.arange(min_coords[1], max_coords[1], grid_spacing)\n",
        "        z_range = np.arange(min_coords[2], max_coords[2], grid_spacing)\n",
        "\n",
        "        binding_sites = []\n",
        "        print(f\"🔍 Scanning {len(x_range) * len(y_range) * len(z_range)} grid points...\")\n",
        "\n",
        "        # Sample fewer points for performance\n",
        "        x_sample = x_range[::2]  # Every other point\n",
        "        y_sample = y_range[::2]\n",
        "        z_sample = z_range[::2]\n",
        "\n",
        "        for i, x in enumerate(x_sample):\n",
        "            if i % 10 == 0:\n",
        "                print(f\"  Progress: {i}/{len(x_sample)} ({100*i/len(x_sample):.0f}%)\")\n",
        "\n",
        "            for y in y_sample:\n",
        "                for z in z_sample:\n",
        "                    grid_point = np.array([x, y, z])\n",
        "\n",
        "                    # Check if point is not too close to any atom (avoid surface)\n",
        "                    distances = np.sqrt(np.sum((coords - grid_point)**2, axis=1))\n",
        "                    min_distance = np.min(distances)\n",
        "\n",
        "                    if min_distance < 2.0 or min_distance > 8.0:  # Skip points too close or too far\n",
        "                        continue\n",
        "\n",
        "                    # Calculate electrostatic potential at this point\n",
        "                    potential = 0.0\n",
        "                    for atom in self.atoms:\n",
        "                        atom_pos = np.array([atom['x'], atom['y'], atom['z']])\n",
        "                        distance = np.linalg.norm(grid_point - atom_pos)\n",
        "                        if distance > 0.1:  # Avoid division by zero\n",
        "                            potential += atom.get('charge', 0.0) / distance\n",
        "\n",
        "                    # Check if this is a favorable binding site\n",
        "                    if potential < pocket_threshold:\n",
        "                        binding_sites.append({\n",
        "                            'position': grid_point,\n",
        "                            'potential': potential,\n",
        "                            'type': 'cation_binding' if potential < -10 else 'weak_binding'\n",
        "                        })\n",
        "\n",
        "        # Cluster nearby binding sites\n",
        "        self.binding_sites = self.cluster_binding_sites(binding_sites)\n",
        "        print(f\"✓ Found {len(self.binding_sites)} potential binding sites\")\n",
        "\n",
        "    def cluster_binding_sites(self, sites: List[Dict], cluster_radius: float = 3.0) -> List[Dict]:\n",
        "        \"\"\"Cluster nearby binding sites into single sites.\"\"\"\n",
        "        if not sites:\n",
        "            return []\n",
        "\n",
        "        positions = np.array([site['position'] for site in sites])\n",
        "        potentials = np.array([site['potential'] for site in sites])\n",
        "\n",
        "        clustered_sites = []\n",
        "        used = np.zeros(len(sites), dtype=bool)\n",
        "\n",
        "        for i, site in enumerate(sites):\n",
        "            if used[i]:\n",
        "                continue\n",
        "\n",
        "            # Find nearby sites\n",
        "            distances = np.sqrt(np.sum((positions - positions[i])**2, axis=1))\n",
        "            cluster_mask = distances < cluster_radius\n",
        "\n",
        "            # Calculate cluster center and average potential\n",
        "            cluster_positions = positions[cluster_mask]\n",
        "            cluster_potentials = potentials[cluster_mask]\n",
        "\n",
        "            center = np.mean(cluster_positions, axis=0)\n",
        "            avg_potential = np.mean(cluster_potentials)\n",
        "            min_potential = np.min(cluster_potentials)\n",
        "\n",
        "            clustered_sites.append({\n",
        "                'position': center,\n",
        "                'potential': avg_potential,\n",
        "                'min_potential': min_potential,\n",
        "                'size': np.sum(cluster_mask),\n",
        "                'type': 'strong_binding' if min_potential < -15 else 'moderate_binding' if min_potential < -8 else 'weak_binding'\n",
        "            })\n",
        "\n",
        "            used[cluster_mask] = True\n",
        "\n",
        "        return clustered_sites\n",
        "\n",
        "    def analyze_charge_interactions(self) -> Dict:\n",
        "        \"\"\"Analyze electrostatic interactions between charged residues.\"\"\"\n",
        "        if not self.charged_residues:\n",
        "            self.identify_charged_residues()\n",
        "\n",
        "        interactions = []\n",
        "\n",
        "        for i, res1 in enumerate(self.charged_residues):\n",
        "            for res2 in self.charged_residues[i+1:]:\n",
        "                distance = np.linalg.norm(res1['center'] - res2['center'])\n",
        "\n",
        "                # Calculate interaction energy (simplified Coulomb)\n",
        "                q1 = res1['net_charge']\n",
        "                q2 = res2['net_charge']\n",
        "                energy = (q1 * q2) / distance if distance > 0 else 0\n",
        "\n",
        "                interaction_type = 'attractive' if energy < 0 else 'repulsive'\n",
        "                strength = 'strong' if abs(energy) > 0.5 else 'moderate' if abs(energy) > 0.1 else 'weak'\n",
        "\n",
        "                interactions.append({\n",
        "                    'residue1': f\"{res1['chain']}-{res1['residue']}{res1['residue_id']}\",\n",
        "                    'residue2': f\"{res2['chain']}-{res2['residue']}{res2['residue_id']}\",\n",
        "                    'distance': distance,\n",
        "                    'energy': energy,\n",
        "                    'type': interaction_type,\n",
        "                    'strength': strength\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            'interactions': interactions,\n",
        "            'total_positive': sum(1 for r in self.charged_residues if r['charge_type'] == 'positive'),\n",
        "            'total_negative': sum(1 for r in self.charged_residues if r['charge_type'] == 'negative'),\n",
        "            'net_charge': sum(r['net_charge'] for r in self.charged_residues)\n",
        "        }\n",
        "\n",
        "    def get_statistics(self) -> Dict:\n",
        "        \"\"\"Get basic statistics about the structure.\"\"\"\n",
        "        if not self.atoms:\n",
        "            return {}\n",
        "\n",
        "        elements = [atom['element'] for atom in self.atoms]\n",
        "        residues = [atom['residue'] for atom in self.atoms]\n",
        "        chains = [atom['chain'] for atom in self.atoms]\n",
        "\n",
        "        coords = np.array([[atom['x'], atom['y'], atom['z']] for atom in self.atoms])\n",
        "\n",
        "        stats = {\n",
        "            'total_atoms': len(self.atoms),\n",
        "            'unique_elements': len(set(elements)),\n",
        "            'unique_residues': len(set(residues)),\n",
        "            'unique_chains': len(set(chains)),\n",
        "            'element_counts': {elem: elements.count(elem) for elem in set(elements)},\n",
        "            'center': np.mean(coords, axis=0),\n",
        "            'size': np.max(coords, axis=0) - np.min(coords, axis=0)\n",
        "        }\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def compile_protein_summary(self, protein_id: str, interactions: Dict) -> Dict:\n",
        "        \"\"\"Compile comprehensive summary for a single protein.\"\"\"\n",
        "        stats = self.get_statistics() if self.atoms else {}\n",
        "\n",
        "        # Binding site analysis\n",
        "        strong_sites = [s for s in self.binding_sites if s.get('type') == 'strong_binding']\n",
        "        moderate_sites = [s for s in self.binding_sites if s.get('type') == 'moderate_binding']\n",
        "\n",
        "        # Charge distribution\n",
        "        positive_residues = [r for r in self.charged_residues if r['charge_type'] == 'positive']\n",
        "        negative_residues = [r for r in self.charged_residues if r['charge_type'] == 'negative']\n",
        "\n",
        "        # Strong interactions\n",
        "        strong_interactions = [i for i in interactions.get('interactions', []) if i['strength'] == 'strong']\n",
        "\n",
        "        # Sequence information\n",
        "        total_sequence_length = sum(data['length'] for data in self.sequence_data.values())\n",
        "\n",
        "        return {\n",
        "            'protein_id': protein_id,\n",
        "            'filename': self.filename,\n",
        "            'analysis_timestamp': datetime.now().isoformat(),\n",
        "\n",
        "            # Basic structure info\n",
        "            'total_atoms': len(self.atoms),\n",
        "            'total_residues': stats.get('unique_residues', 0),\n",
        "            'total_chains': stats.get('unique_chains', 0),\n",
        "            'sequence_length': total_sequence_length,\n",
        "            'sequence_data': self.sequence_data,\n",
        "\n",
        "            # Charge analysis\n",
        "            'total_charged_residues': len(self.charged_residues),\n",
        "            'positive_residues': len(positive_residues),\n",
        "            'negative_residues': len(negative_residues),\n",
        "            'net_protein_charge': sum(r['net_charge'] for r in self.charged_residues),\n",
        "            'total_molecular_charge': sum(atom.get('charge', 0.0) for atom in self.atoms),\n",
        "\n",
        "            # Binding sites\n",
        "            'total_binding_sites': len(self.binding_sites),\n",
        "            'strong_binding_sites': len(strong_sites),\n",
        "            'moderate_binding_sites': len(moderate_sites),\n",
        "            'best_binding_potential': min([s['potential'] for s in self.binding_sites]) if self.binding_sites else 0,\n",
        "\n",
        "            # Interactions\n",
        "            'total_interactions': len(interactions.get('interactions', [])),\n",
        "            'strong_interactions': len(strong_interactions),\n",
        "\n",
        "            # Detailed residue breakdown\n",
        "            'positive_residue_types': {res_type: len([r for r in positive_residues if r['residue'] == res_type])\n",
        "                                    for res_type in ['ARG', 'LYS', 'HIS']},\n",
        "            'negative_residue_types': {res_type: len([r for r in negative_residues if r['residue'] == res_type])\n",
        "                                     for res_type in ['ASP', 'GLU']},\n",
        "\n",
        "            # Top binding sites (up to 5)\n",
        "            'top_binding_sites': sorted(self.binding_sites, key=lambda x: x['potential'])[:5] if self.binding_sites else [],\n",
        "\n",
        "            # Key interactions\n",
        "            'key_interactions': strong_interactions[:10] if strong_interactions else [],\n",
        "\n",
        "            # Charge distribution stats\n",
        "            'charge_stats': {\n",
        "                'min_charge': min([atom.get('charge', 0.0) for atom in self.atoms]) if self.atoms else 0,\n",
        "                'max_charge': max([atom.get('charge', 0.0) for atom in self.atoms]) if self.atoms else 0,\n",
        "                'charge_std': np.std([atom.get('charge', 0.0) for atom in self.atoms]) if self.atoms else 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def batch_analyze_folder_realtime_with_resume(self, folder_path: str, output_folder: str = None, resume: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        Enhanced batch analyze with RESUME capability - picks up where it left off.\n",
        "\n",
        "        Args:\n",
        "            folder_path: Path to folder containing .pdb.gz files\n",
        "            output_folder: Path to output folder (default: folder_path/charge_analysis_results)\n",
        "            resume: If True, attempts to resume from previous run\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with summary statistics across all analyzed proteins\n",
        "        \"\"\"\n",
        "\n",
        "        if not os.path.exists(folder_path):\n",
        "            print(f\"✗ Folder not found: {folder_path}\")\n",
        "            return {}\n",
        "\n",
        "        # Set up output directory\n",
        "        if output_folder is None:\n",
        "            output_folder = os.path.join(folder_path, \"charge_analysis_results\")\n",
        "\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "        # Find all .pdb.gz files\n",
        "        pdb_files = glob.glob(os.path.join(folder_path, \"*.pdb.gz\"))\n",
        "        pdb_files.extend(glob.glob(os.path.join(folder_path, \"*.pdb\")))\n",
        "\n",
        "        if not pdb_files:\n",
        "            print(f\"✗ No .pdb or .pdb.gz files found in {folder_path}\")\n",
        "            return {}\n",
        "\n",
        "        print(f\"🔍 Found {len(pdb_files)} PDB files to analyze\")\n",
        "\n",
        "        # ═══════════════════════════════════════════════════════\n",
        "        # RESUME LOGIC - Check for existing progress\n",
        "        # ═══════════════════════════════════════════════════════\n",
        "        completed_proteins = set()\n",
        "        failed_proteins = set()\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        existing_timestamp = None\n",
        "\n",
        "        if resume:\n",
        "            # Look for existing progress files\n",
        "            existing_progress_files = glob.glob(os.path.join(output_folder, \"analysis_progress_*.csv\"))\n",
        "\n",
        "            if existing_progress_files:\n",
        "                # Find the most recent progress file\n",
        "                latest_progress_file = max(existing_progress_files, key=os.path.getmtime)\n",
        "                existing_timestamp = latest_progress_file.split('_')[-1].replace('.csv', '')\n",
        "\n",
        "                print(f\"📋 Found existing progress file: {os.path.basename(latest_progress_file)}\")\n",
        "\n",
        "                try:\n",
        "                    import pandas as pd\n",
        "                    progress_df = pd.read_csv(latest_progress_file)\n",
        "\n",
        "                    completed_proteins = set(progress_df[progress_df['status'] == 'COMPLETED']['protein_id'].tolist())\n",
        "                    failed_proteins = set(progress_df[progress_df['status'] == 'FAILED']['protein_id'].tolist())\n",
        "\n",
        "                    print(f\"✅ Previously completed: {len(completed_proteins)} proteins\")\n",
        "                    print(f\"❌ Previously failed: {len(failed_proteins)} proteins\")\n",
        "\n",
        "                    if completed_proteins:\n",
        "                        print(f\"📄 Last completed: {list(completed_proteins)[-5:]}\")\n",
        "\n",
        "                    # Use existing timestamp to continue same batch\n",
        "                    timestamp = existing_timestamp\n",
        "                    print(f\"🔄 Resuming batch with timestamp: {timestamp}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error reading progress file: {e}\")\n",
        "                    print(\"🔄 Starting fresh analysis\")\n",
        "                    resume = False\n",
        "\n",
        "        # Filter out already completed proteins\n",
        "        remaining_files = []\n",
        "        skipped_completed = 0\n",
        "        skipped_failed = 0\n",
        "\n",
        "        for filepath in pdb_files:\n",
        "            filename = os.path.basename(filepath)\n",
        "            protein_id = filename.split('.')[0].upper()\n",
        "\n",
        "            if protein_id in completed_proteins:\n",
        "                skipped_completed += 1\n",
        "                continue\n",
        "            elif protein_id in failed_proteins:\n",
        "                print(f\"⚠️ Skipping previously failed protein: {protein_id}\")\n",
        "                skipped_failed += 1\n",
        "                continue\n",
        "            else:\n",
        "                remaining_files.append(filepath)\n",
        "\n",
        "        print(f\"📊 Analysis Summary:\")\n",
        "        print(f\"  📁 Total files found: {len(pdb_files)}\")\n",
        "        print(f\"  ✅ Already completed: {skipped_completed}\")\n",
        "        print(f\"  ❌ Previously failed: {skipped_failed}\")\n",
        "        print(f\"  🔄 Remaining to process: {len(remaining_files)}\")\n",
        "\n",
        "        if not remaining_files:\n",
        "            print(\"🎉 All proteins already completed!\")\n",
        "            return {\n",
        "                'analyzed': len(completed_proteins),\n",
        "                'failed': len(failed_proteins),\n",
        "                'output_folder': output_folder,\n",
        "                'results': [],\n",
        "                'already_completed': True\n",
        "            }\n",
        "\n",
        "        print(f\"📁 Output will be saved to: {output_folder}\")\n",
        "\n",
        "        # Setup real-time writers (will append to existing files if resuming)\n",
        "        if resume and existing_timestamp:\n",
        "            # Use existing timestamp and append mode\n",
        "            self.setup_realtime_writers_resume(output_folder, timestamp, append=True)\n",
        "        else:\n",
        "            # Create new files\n",
        "            self.setup_realtime_writers(output_folder, timestamp)\n",
        "\n",
        "        # Initialize batch results\n",
        "        batch_results = []\n",
        "        failed_files = []\n",
        "\n",
        "        try:\n",
        "            # Process remaining files\n",
        "            for i, filepath in enumerate(remaining_files):\n",
        "                filename = os.path.basename(filepath)\n",
        "                protein_id = filename.split('.')[0].upper()\n",
        "\n",
        "                print(f\"\\n📊 Processing {i+1}/{len(remaining_files)}: {protein_id} (Overall: {skipped_completed + skipped_failed + i + 1}/{len(pdb_files)})\")\n",
        "                start_time = time.time()\n",
        "\n",
        "                try:\n",
        "                    # Write initial progress\n",
        "                    self.write_progress_update(protein_id, filename, \"STARTED\")\n",
        "\n",
        "                    # Reset analyzer for new structure\n",
        "                    self.atoms = []\n",
        "                    self.charged_residues = []\n",
        "                    self.binding_sites = []\n",
        "                    self.sequence_data = {}\n",
        "\n",
        "                    # Load and parse structure\n",
        "                    content = self.open_file(filepath)\n",
        "                    if not content:\n",
        "                        error_msg = \"Failed to read file\"\n",
        "                        failed_files.append((filename, error_msg))\n",
        "                        self.write_progress_update(protein_id, filename, \"FAILED\", error_msg)\n",
        "                        continue\n",
        "\n",
        "                    if filepath.lower().endswith(('.pdb.gz', '.pdb')):\n",
        "                        self.parse_pdb(content)\n",
        "                    else:\n",
        "                        error_msg = \"Unsupported format\"\n",
        "                        failed_files.append((filename, error_msg))\n",
        "                        self.write_progress_update(protein_id, filename, \"FAILED\", error_msg)\n",
        "                        continue\n",
        "\n",
        "                    if not self.atoms:\n",
        "                        error_msg = \"No atoms parsed\"\n",
        "                        failed_files.append((filename, error_msg))\n",
        "                        self.write_progress_update(protein_id, filename, \"FAILED\", error_msg)\n",
        "                        continue\n",
        "\n",
        "                    # Write sequence data immediately after parsing\n",
        "                    self.write_sequence_data(protein_id)\n",
        "                    self.write_progress_update(protein_id, filename, \"SEQUENCES_EXTRACTED\")\n",
        "\n",
        "                    # Perform charge analysis\n",
        "                    print(\"  ⚡ Assigning charges...\")\n",
        "                    self.assign_partial_charges()\n",
        "                    self.write_progress_update(protein_id, filename, \"CHARGES_ASSIGNED\")\n",
        "\n",
        "                    print(\"  🔍 Identifying charged residues...\")\n",
        "                    self.identify_charged_residues()\n",
        "                    self.write_progress_update(protein_id, filename, \"CHARGED_RESIDUES_IDENTIFIED\")\n",
        "\n",
        "                    print(\"  🔗 Analyzing interactions...\")\n",
        "                    interactions = self.analyze_charge_interactions()\n",
        "                    self.write_interactions_data(protein_id, interactions)\n",
        "                    self.write_progress_update(protein_id, filename, \"INTERACTIONS_ANALYZED\")\n",
        "\n",
        "                    # Calculate binding pockets (quick version)\n",
        "                    print(\"  🎯 Finding binding sites...\")\n",
        "                    self.find_binding_pockets(grid_spacing=3.0, pocket_threshold=-3.0)\n",
        "                    self.write_binding_sites_data(protein_id)\n",
        "                    self.write_progress_update(protein_id, filename, \"BINDING_SITES_FOUND\")\n",
        "\n",
        "                    # Compile and write summary\n",
        "                    protein_result = self.compile_protein_summary(protein_id, interactions)\n",
        "                    self.write_summary_data(protein_result)\n",
        "                    batch_results.append(protein_result)\n",
        "\n",
        "                    # Final progress update\n",
        "                    processing_time = time.time() - start_time\n",
        "                    self.write_progress_update(protein_id, filename, \"COMPLETED\", \"\", processing_time)\n",
        "\n",
        "                    print(f\"  ✅ {protein_id}: {len(self.atoms)} atoms, {len(self.charged_residues)} charged residues, \"\n",
        "                          f\"{len(self.binding_sites)} binding sites, {sum(len(seq['final_sequence']) for seq in self.sequence_data.values())} total sequence length\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    processing_time = time.time() - start_time\n",
        "                    error_msg = f\"Analysis failed: {str(e)}\"\n",
        "                    failed_files.append((filename, error_msg))\n",
        "                    self.write_progress_update(protein_id, filename, \"FAILED\", error_msg, processing_time)\n",
        "                    print(f\"  ✗ {protein_id}: {error_msg}\")\n",
        "                    continue\n",
        "\n",
        "        finally:\n",
        "            # Always close writers, even if there's an error\n",
        "            self.close_realtime_writers()\n",
        "\n",
        "        # Generate final consolidated reports (only if new proteins were processed)\n",
        "        if batch_results:\n",
        "            json_file = os.path.join(output_folder, f\"charge_analysis_detailed_{timestamp}.json\")\n",
        "            self.export_final_json_report(batch_results, failed_files, json_file)\n",
        "\n",
        "            report_file = os.path.join(output_folder, f\"charge_analysis_report_{timestamp}.txt\")\n",
        "            self.export_final_text_report(batch_results, failed_files, report_file)\n",
        "\n",
        "        total_analyzed = len(completed_proteins) + len(batch_results)\n",
        "        total_failed = len(failed_proteins) + len(failed_files)\n",
        "\n",
        "        print(f\"\\n🎉 Batch analysis complete!\")\n",
        "        print(f\"  ✅ Total analyzed: {total_analyzed} proteins ({len(batch_results)} new)\")\n",
        "        print(f\"  ✗ Total failed: {total_failed} proteins ({len(failed_files)} new)\")\n",
        "        print(f\"  📁 Results saved to: {output_folder}\")\n",
        "        if resume and skipped_completed > 0:\n",
        "            print(f\"  ⏭️  Skipped {skipped_completed} already completed proteins\")\n",
        "\n",
        "        return {\n",
        "            'analyzed': total_analyzed,\n",
        "            'failed': total_failed,\n",
        "            'new_analyzed': len(batch_results),\n",
        "            'new_failed': len(failed_files),\n",
        "            'skipped_completed': skipped_completed,\n",
        "            'skipped_failed': skipped_failed,\n",
        "            'output_folder': output_folder,\n",
        "            'results': batch_results,\n",
        "            'realtime_files': {\n",
        "                'summary': f\"charge_analysis_summary_{timestamp}.csv\",\n",
        "                'sequences': f\"protein_sequences_{timestamp}.csv\",\n",
        "                'binding_sites': f\"binding_sites_{timestamp}.csv\",\n",
        "                'interactions': f\"charge_interactions_{timestamp}.csv\",\n",
        "                'progress': f\"analysis_progress_{timestamp}.csv\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def export_final_json_report(self, results: List[Dict], failed_files: List, filepath: str) -> None:\n",
        "        \"\"\"Export detailed JSON report with all analysis data including sequences.\"\"\"\n",
        "        report_data = {\n",
        "            'analysis_metadata': {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'total_proteins_analyzed': len(results),\n",
        "                'total_failed': len(failed_files),\n",
        "                'analysis_version': '2.0_with_sequences_and_resume',\n",
        "                'features': [\n",
        "                    'real_time_data_capture',\n",
        "                    'sequence_extraction',\n",
        "                    'electrostatic_analysis',\n",
        "                    'binding_site_prediction',\n",
        "                    'charge_interaction_analysis',\n",
        "                    'resume_capability'\n",
        "                ]\n",
        "            },\n",
        "            'failed_files': [{'filename': f[0], 'error': f[1]} for f in failed_files],\n",
        "            'protein_analyses': results,\n",
        "            'summary_statistics': self.calculate_batch_statistics(results) if results else {}\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'w') as jsonfile:\n",
        "            json.dump(report_data, jsonfile, indent=2, default=str)\n",
        "\n",
        "        print(f\"  📋 Final detailed JSON saved: {filepath}\")\n",
        "\n",
        "    def export_final_text_report(self, results: List[Dict], failed_files: List, filepath: str) -> None:\n",
        "        \"\"\"Export enhanced human-readable text report with sequence information.\"\"\"\n",
        "        with open(filepath, 'w') as f:\n",
        "            f.write(\"ENHANCED PROTEIN CHARGE AFFINITY ANALYSIS REPORT (WITH RESUME)\\n\")\n",
        "            f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "            f.write(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"Total Proteins Analyzed: {len(results)}\\n\")\n",
        "            f.write(f\"Failed Analyses: {len(failed_files)}\\n\")\n",
        "            f.write(\"Features: Real-time data capture, Sequence extraction, Resume capability\\n\\n\")\n",
        "\n",
        "            if failed_files:\n",
        "                f.write(\"FAILED ANALYSES:\\n\")\n",
        "                f.write(\"-\" * 20 + \"\\n\")\n",
        "                for filename, error in failed_files:\n",
        "                    f.write(f\"  {filename}: {error}\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            f.write(\"SUMMARY STATISTICS:\\n\")\n",
        "            f.write(\"-\" * 20 + \"\\n\")\n",
        "            if results:\n",
        "                stats = self.calculate_batch_statistics(results)\n",
        "                f.write(f\"  Average net charge: {stats['avg_net_charge']:.2f}\\n\")\n",
        "                f.write(f\"  Average sequence length: {stats['avg_sequence_length']:.1f}\\n\")\n",
        "                f.write(f\"  Average binding sites: {stats['avg_binding_sites']:.1f}\\n\")\n",
        "                f.write(f\"  Proteins with strong binding sites: {stats['proteins_with_strong_sites']}\\n\")\n",
        "                f.write(f\"  Total sequence length analyzed: {stats['total_sequence_length']}\\n\")\n",
        "                f.write(f\"  Most common charged residues: {stats['most_common_charged_residues']}\\n\\n\")\n",
        "\n",
        "            f.write(\"INDIVIDUAL PROTEIN RESULTS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            for result in results:\n",
        "                f.write(f\"\\nProtein ID: {result['protein_id']}\\n\")\n",
        "                f.write(f\"  Filename: {result['filename']}\\n\")\n",
        "                f.write(f\"  Total atoms: {result['total_atoms']}\\n\")\n",
        "                f.write(f\"  Total chains: {result['total_chains']}\\n\")\n",
        "                f.write(f\"  Sequence length: {result['sequence_length']} residues\\n\")\n",
        "                f.write(f\"  Charged residues: {result['total_charged_residues']} (+{result['positive_residues']}/-{result['negative_residues']})\\n\")\n",
        "                f.write(f\"  Net charge: {result['net_protein_charge']:.2f}\\n\")\n",
        "                f.write(f\"  Binding sites: {result['total_binding_sites']} ({result['strong_binding_sites']} strong)\\n\")\n",
        "                f.write(f\"  Strong interactions: {result['strong_interactions']}\\n\")\n",
        "\n",
        "                # Sequence information per chain\n",
        "                if result['sequence_data']:\n",
        "                    f.write(f\"  Chain sequences:\\n\")\n",
        "                    for chain_id, seq_data in result['sequence_data'].items():\n",
        "                        f.write(f\"    Chain {chain_id}: {seq_data['length']} residues\\n\")\n",
        "\n",
        "                if result['top_binding_sites']:\n",
        "                    f.write(f\"  Best binding potential: {result['best_binding_potential']:.2f}\\n\")\n",
        "\n",
        "        print(f\"  📄 Final text report saved: {filepath}\")\n",
        "\n",
        "    def calculate_batch_statistics(self, results: List[Dict]) -> Dict:\n",
        "        \"\"\"Calculate comprehensive statistics across all analyzed proteins.\"\"\"\n",
        "        if not results:\n",
        "            return {}\n",
        "\n",
        "        # Basic statistics\n",
        "        net_charges = [r['net_protein_charge'] for r in results]\n",
        "        sequence_lengths = [r['sequence_length'] for r in results]\n",
        "        binding_sites = [r['total_binding_sites'] for r in results]\n",
        "\n",
        "        # Charged residue counts\n",
        "        all_charged_residues = []\n",
        "        for result in results:\n",
        "            for res_type, count in result['positive_residue_types'].items():\n",
        "                all_charged_residues.extend([res_type] * count)\n",
        "            for res_type, count in result['negative_residue_types'].items():\n",
        "                all_charged_residues.extend([res_type] * count)\n",
        "\n",
        "        # Count occurrences\n",
        "        from collections import Counter\n",
        "        residue_counts = Counter(all_charged_residues)\n",
        "\n",
        "        return {\n",
        "            'avg_net_charge': np.mean(net_charges),\n",
        "            'std_net_charge': np.std(net_charges),\n",
        "            'avg_sequence_length': np.mean(sequence_lengths),\n",
        "            'std_sequence_length': np.std(sequence_lengths),\n",
        "            'avg_binding_sites': np.mean(binding_sites),\n",
        "            'proteins_with_strong_sites': len([r for r in results if r['strong_binding_sites'] > 0]),\n",
        "            'total_sequence_length': sum(sequence_lengths),\n",
        "            'total_proteins': len(results),\n",
        "            'most_common_charged_residues': dict(residue_counts.most_common(5)),\n",
        "            'charge_distribution': {\n",
        "                'positive_proteins': len([r for r in results if r['net_protein_charge'] > 0]),\n",
        "                'negative_proteins': len([r for r in results if r['net_protein_charge'] < 0]),\n",
        "                'neutral_proteins': len([r for r in results if abs(r['net_protein_charge']) < 0.1])\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def visualize_charge_distribution(self) -> None:\n",
        "        \"\"\"Visualize charge distribution and binding sites with sequence information.\"\"\"\n",
        "        if not self.atoms:\n",
        "            print(\"No structure loaded!\")\n",
        "            return\n",
        "\n",
        "        if not hasattr(self.atoms[0], 'charge') or 'charge' not in self.atoms[0]:\n",
        "            self.assign_partial_charges()\n",
        "\n",
        "        if not self.charged_residues:\n",
        "            self.identify_charged_residues()\n",
        "\n",
        "        # Check if we have enough data to plot\n",
        "        if len(self.atoms) == 0:\n",
        "            print(\"⚠️  No atoms to visualize!\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            fig = plt.figure(figsize=(18, 14))\n",
        "\n",
        "            # Main 3D plot\n",
        "            ax1 = fig.add_subplot(231, projection='3d')\n",
        "\n",
        "            # Plot all atoms (faded)\n",
        "            coords = np.array([[atom['x'], atom['y'], atom['z']] for atom in self.atoms])\n",
        "            ax1.scatter(coords[:, 0], coords[:, 1], coords[:, 2],\n",
        "                       c='lightgray', alpha=0.3, s=10, label='atoms')\n",
        "\n",
        "            # Plot charged residues with proper legend handling\n",
        "            legend_labels_added = set()\n",
        "            if self.charged_residues:\n",
        "                for residue in self.charged_residues:\n",
        "                    color = 'red' if residue['charge_type'] == 'positive' else 'blue' if residue['charge_type'] == 'negative' else 'green'\n",
        "                    size = abs(residue['net_charge']) * 100 + 50\n",
        "\n",
        "                    # Add label only for first occurrence of each charge type\n",
        "                    label = residue['charge_type'] if residue['charge_type'] not in legend_labels_added else \"\"\n",
        "                    if label:\n",
        "                        legend_labels_added.add(label)\n",
        "\n",
        "                    ax1.scatter(residue['center'][0], residue['center'][1], residue['center'][2],\n",
        "                               c=color, s=size, alpha=0.8, label=label)\n",
        "\n",
        "            # Plot binding sites\n",
        "            if self.binding_sites:\n",
        "                binding_labels_added = set()\n",
        "                for site in self.binding_sites:\n",
        "                    color = 'yellow' if site['type'] == 'strong_binding' else 'orange'\n",
        "\n",
        "                    label = site['type'] if site['type'] not in binding_labels_added else \"\"\n",
        "                    if label:\n",
        "                        binding_labels_added.add(label)\n",
        "\n",
        "                    ax1.scatter(site['position'][0], site['position'][1], site['position'][2],\n",
        "                               c=color, marker='s', s=100, alpha=0.7, label=label)\n",
        "\n",
        "            ax1.set_xlabel('X (Å)')\n",
        "            ax1.set_ylabel('Y (Å)')\n",
        "            ax1.set_zlabel('Z (Å)')\n",
        "            ax1.set_title(f'Charge Distribution & Binding Sites\\n{self.filename}')\n",
        "\n",
        "            # Add legend if we have labeled items\n",
        "            handles, labels = ax1.get_legend_handles_labels()\n",
        "            if labels:\n",
        "                ax1.legend()\n",
        "\n",
        "            # Charge distribution histogram\n",
        "            ax2 = fig.add_subplot(232)\n",
        "            charges = [atom.get('charge', 0.0) for atom in self.atoms if 'charge' in atom]\n",
        "            if charges:\n",
        "                ax2.hist(charges, bins=50, alpha=0.7, color='skyblue')\n",
        "                ax2.set_xlabel('Partial Charge')\n",
        "                ax2.set_ylabel('Number of Atoms')\n",
        "                ax2.set_title('Charge Distribution')\n",
        "\n",
        "            # Residue charge analysis\n",
        "            ax3 = fig.add_subplot(233)\n",
        "            if self.charged_residues:\n",
        "                charge_types = [r['charge_type'] for r in self.charged_residues]\n",
        "                type_counts = {t: charge_types.count(t) for t in set(charge_types)}\n",
        "                colors = {'positive': 'red', 'negative': 'blue', 'variable': 'green'}\n",
        "                bar_colors = [colors.get(t, 'gray') for t in type_counts.keys()]\n",
        "                ax3.bar(type_counts.keys(), type_counts.values(), color=bar_colors, alpha=0.7)\n",
        "                ax3.set_ylabel('Count')\n",
        "                ax3.set_title('Charged Residue Types')\n",
        "\n",
        "            # Binding site potential\n",
        "            ax4 = fig.add_subplot(234)\n",
        "            if self.binding_sites:\n",
        "                potentials = [site['potential'] for site in self.binding_sites]\n",
        "                ax4.hist(potentials, bins=20, alpha=0.7, color='orange')\n",
        "                ax4.set_xlabel('Electrostatic Potential')\n",
        "                ax4.set_ylabel('Number of Sites')\n",
        "                ax4.set_title('Binding Site Potentials')\n",
        "\n",
        "            # Sequence length per chain\n",
        "            ax5 = fig.add_subplot(235)\n",
        "            if self.sequence_data:\n",
        "                chain_ids = list(self.sequence_data.keys())\n",
        "                chain_lengths = [self.sequence_data[chain]['length'] for chain in chain_ids]\n",
        "                ax5.bar(chain_ids, chain_lengths, alpha=0.7, color='green')\n",
        "                ax5.set_xlabel('Chain ID')\n",
        "                ax5.set_ylabel('Sequence Length')\n",
        "                ax5.set_title('Chain Sequence Lengths')\n",
        "\n",
        "            # Overall statistics\n",
        "            ax6 = fig.add_subplot(236)\n",
        "            ax6.axis('off')\n",
        "            stats_text = f\"Structure Statistics:\\n\"\n",
        "            stats_text += f\"Total atoms: {len(self.atoms)}\\n\"\n",
        "            stats_text += f\"Total chains: {len(self.sequence_data)}\\n\"\n",
        "            if self.sequence_data:\n",
        "                total_seq_len = sum(data['length'] for data in self.sequence_data.values())\n",
        "                stats_text += f\"Total sequence length: {total_seq_len}\\n\"\n",
        "            stats_text += f\"Charged residues: {len(self.charged_residues)}\\n\"\n",
        "            stats_text += f\"Binding sites: {len(self.binding_sites)}\\n\"\n",
        "\n",
        "            if self.sequence_data:\n",
        "                stats_text += f\"\\nSequence Information:\\n\"\n",
        "                for chain_id, seq_data in self.sequence_data.items():\n",
        "                    stats_text += f\"Chain {chain_id}: {seq_data['length']} residues\\n\"\n",
        "\n",
        "            ax6.text(0.1, 0.9, stats_text, transform=ax6.transAxes, fontsize=10,\n",
        "                    verticalalignment='top', fontfamily='monospace')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Error creating visualization: {e}\")\n",
        "\n",
        "    def print_enhanced_analysis(self) -> None:\n",
        "        \"\"\"Print comprehensive charge analysis with sequence information.\"\"\"\n",
        "        if not self.atoms:\n",
        "            print(\"No structure loaded!\")\n",
        "            return\n",
        "\n",
        "        if not hasattr(self.atoms[0], 'charge') or 'charge' not in self.atoms[0]:\n",
        "            self.assign_partial_charges()\n",
        "\n",
        "        if not self.charged_residues:\n",
        "            self.identify_charged_residues()\n",
        "\n",
        "        print(f\"\\n⚡ Enhanced Charge Affinity Analysis for: {self.filename}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Sequence information\n",
        "        if self.sequence_data:\n",
        "            print(f\"\\n🧬 Sequence Information:\")\n",
        "            total_length = 0\n",
        "            for chain_id, seq_data in self.sequence_data.items():\n",
        "                print(f\"  Chain {chain_id}: {seq_data['length']} residues\")\n",
        "                total_length += seq_data['length']\n",
        "                if seq_data['length'] <= 50:  # Show short sequences\n",
        "                    print(f\"    Sequence: {seq_data['final_sequence']}\")\n",
        "            print(f\"  Total sequence length: {total_length} residues\")\n",
        "\n",
        "        # Overall statistics\n",
        "        total_charge = sum(atom.get('charge', 0.0) for atom in self.atoms)\n",
        "        print(f\"\\n⚡ Charge Analysis:\")\n",
        "        print(f\"  Total molecular charge: {total_charge:.2f}\")\n",
        "        print(f\"  Total charged residues: {len(self.charged_residues)}\")\n",
        "\n",
        "        # Charged residue breakdown\n",
        "        positive_residues = [r for r in self.charged_residues if r['charge_type'] == 'positive']\n",
        "        negative_residues = [r for r in self.charged_residues if r['charge_type'] == 'negative']\n",
        "\n",
        "        print(f\"\\n📊 Charged Residue Distribution:\")\n",
        "        print(f\"  Positive residues: {len(positive_residues)} (ARG, LYS)\")\n",
        "        print(f\"  Negative residues: {len(negative_residues)} (ASP, GLU)\")\n",
        "        print(f\"  Net charge from residues: {sum(r['net_charge'] for r in self.charged_residues):.1f}\")\n",
        "\n",
        "        # Binding sites\n",
        "        if self.binding_sites:\n",
        "            print(f\"\\n🎯 Potential Binding Sites: {len(self.binding_sites)}\")\n",
        "            strong_sites = [s for s in self.binding_sites if s['type'] == 'strong_binding']\n",
        "            moderate_sites = [s for s in self.binding_sites if s['type'] == 'moderate_binding']\n",
        "\n",
        "            print(f\"  Strong binding sites: {len(strong_sites)}\")\n",
        "            print(f\"  Moderate binding sites: {len(moderate_sites)}\")\n",
        "\n",
        "            if strong_sites:\n",
        "                print(f\"\\n🏆 Top Binding Sites:\")\n",
        "                sorted_sites = sorted(self.binding_sites, key=lambda x: x['potential'])\n",
        "                for i, site in enumerate(sorted_sites[:5]):\n",
        "                    print(f\"  Site {i+1}: Potential {site['potential']:.1f} at ({site['position'][0]:.1f}, {site['position'][1]:.1f}, {site['position'][2]:.1f})\")\n",
        "\n",
        "        # Interaction analysis\n",
        "        interactions = self.analyze_charge_interactions()\n",
        "        strong_interactions = [i for i in interactions['interactions'] if i['strength'] == 'strong']\n",
        "\n",
        "        print(f\"\\n🔗 Electrostatic Interactions:\")\n",
        "        print(f\"  Total interactions analyzed: {len(interactions['interactions'])}\")\n",
        "        print(f\"  Strong interactions: {len(strong_interactions)}\")\n",
        "\n",
        "        if strong_interactions:\n",
        "            print(f\"\\n⚡ Key Interactions:\")\n",
        "            for interaction in strong_interactions[:5]:\n",
        "                print(f\"  {interaction['residue1']} ↔ {interaction['residue2']}: \"\n",
        "                      f\"{interaction['distance']:.1f}Å, {interaction['type']}\")"
      ],
      "metadata": {
        "id": "L8cCR1i9rCYp"
      },
      "id": "L8cCR1i9rCYp",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d6c0337",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d6c0337",
        "outputId": "d0d9635e-acf0-4270-d0ff-933ac8e85434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ Enhanced Molecular Charge Affinity Analyzer v2.1\n",
            "=================================================================\n",
            "Features:\n",
            "✓ Real-time data capture during analysis\n",
            "✓ Protein sequence extraction and analysis\n",
            "✓ Enhanced reporting with sequence information\n",
            "✓ Progress tracking and error handling\n",
            "✓ RESUME capability - picks up where it left off!\n",
            "\n",
            "Options:\n",
            "1. Analyze single file (enhanced)\n",
            "2. Batch analyze folder (real-time with resume)\n",
            "3. Exit\n",
            "\n",
            "Select mode (1-3): 2\n",
            "Enter folder path containing .pdb/.pdb.gz files: /content/drive/MyDrive/IontheFold/downloads/ElectronMicroscope/batch_01\n",
            "Enter output folder (press Enter for default): /content/drive/MyDrive/IontheFold/EDA/ElectronMicroscope/1-1000\n",
            "Resume from previous run if possible? (Y/n): n\n",
            "\n",
            "🆕 Starting fresh batch analysis...\n",
            "🔍 Found 460 PDB files to analyze\n",
            "📊 Analysis Summary:\n",
            "  📁 Total files found: 460\n",
            "  ✅ Already completed: 0\n",
            "  ❌ Previously failed: 0\n",
            "  🔄 Remaining to process: 460\n",
            "📁 Output will be saved to: /content/drive/MyDrive/IontheFold/EDA/ElectronMicroscope/1-1000\n",
            "✓ Setup real-time writer: /content/drive/MyDrive/IontheFold/EDA/ElectronMicroscope/1-1000/charge_analysis_summary_20250819_085536.csv\n",
            "✓ Setup real-time writer: /content/drive/MyDrive/IontheFold/EDA/ElectronMicroscope/1-1000/protein_sequences_20250819_085536.csv\n",
            "✓ Setup real-time writer: /content/drive/MyDrive/IontheFold/EDA/ElectronMicroscope/1-1000/binding_sites_20250819_085536.csv\n",
            "✓ Setup real-time writer: /content/drive/MyDrive/IontheFold/EDA/ElectronMicroscope/1-1000/charge_interactions_20250819_085536.csv\n",
            "✓ Setup real-time writer: /content/drive/MyDrive/IontheFold/EDA/ElectronMicroscope/1-1000/analysis_progress_20250819_085536.csv\n",
            "\n",
            "📊 Processing 1/460: 5T4D (Overall: 1/460)\n",
            "✓ Successfully opened compressed file: 5T4D.pdb.gz\n",
            "✓ Extracted sequences for 4 chains\n",
            "  Chain A: 510 residues\n",
            "  Chain B: 510 residues\n",
            "  Chain C: 510 residues\n",
            "  Chain D: 510 residues\n",
            "✓ Parsed 14876 atoms from PDB file\n",
            "  ⚡ Assigning charges...\n",
            "✓ Assigned partial charges to 14876 atoms\n",
            "  🔍 Identifying charged residues...\n",
            "✓ Identified 336 charged residues\n",
            "  🔗 Analyzing interactions...\n",
            "  🎯 Finding binding sites...\n",
            "⚠️  No charges assigned. Running charge assignment first...\n",
            "✓ Assigned partial charges to 14876 atoms\n",
            "🔍 Scanning 46546 grid points...\n",
            "  Progress: 0/19 (0%)\n",
            "  Progress: 10/19 (53%)\n",
            "✓ Found 1929 potential binding sites\n",
            "  ✅ 5T4D: 14876 atoms, 336 charged residues, 1929 binding sites, 2040 total sequence length\n",
            "\n",
            "📊 Processing 2/460: 5OSG (Overall: 2/460)\n",
            "✓ Successfully opened compressed file: 5OSG.pdb.gz\n",
            "✓ Extracted sequences for 3 chains\n",
            "  Chain h: 235 residues\n",
            "  Chain 2: 2205 residues\n",
            "  Chain P: 249 residues\n",
            "✓ Parsed 3465 atoms from PDB file\n",
            "  ⚡ Assigning charges...\n",
            "✓ Assigned partial charges to 3465 atoms\n",
            "  🔍 Identifying charged residues...\n",
            "✓ Identified 59 charged residues\n",
            "  🔗 Analyzing interactions...\n",
            "  🎯 Finding binding sites...\n",
            "⚠️  No charges assigned. Running charge assignment first...\n",
            "✓ Assigned partial charges to 3465 atoms\n",
            "🔍 Scanning 18096 grid points...\n",
            "  Progress: 0/15 (0%)\n",
            "  Progress: 10/15 (67%)\n",
            "✓ Found 357 potential binding sites\n",
            "  ✅ 5OSG: 3465 atoms, 59 charged residues, 357 binding sites, 2689 total sequence length\n",
            "\n",
            "📊 Processing 3/460: 5L35 (Overall: 3/460)\n",
            "✓ Successfully opened compressed file: 5L35.pdb.gz\n",
            "✓ Extracted sequences for 7 chains\n",
            "  Chain A: 422 residues\n",
            "  Chain B: 422 residues\n",
            "  Chain C: 422 residues\n",
            "  Chain D: 422 residues\n",
            "  Chain E: 422 residues\n",
            "  Chain F: 422 residues\n",
            "  Chain G: 422 residues\n",
            "✓ Parsed 22464 atoms from PDB file\n",
            "  ⚡ Assigning charges...\n",
            "✓ Assigned partial charges to 22464 atoms\n",
            "  🔍 Identifying charged residues...\n",
            "✓ Identified 511 charged residues\n",
            "  🔗 Analyzing interactions...\n",
            "  🎯 Finding binding sites...\n",
            "⚠️  No charges assigned. Running charge assignment first...\n",
            "✓ Assigned partial charges to 22464 atoms\n",
            "🔍 Scanning 179630 grid points...\n",
            "  Progress: 0/28 (0%)\n",
            "  Progress: 10/28 (36%)\n",
            "  Progress: 20/28 (71%)\n",
            "✓ Found 3188 potential binding sites\n",
            "  ✅ 5L35: 22464 atoms, 511 charged residues, 3188 binding sites, 2954 total sequence length\n",
            "\n",
            "📊 Processing 4/460: 5K12 (Overall: 4/460)\n",
            "✓ Successfully opened compressed file: 5K12.pdb.gz\n",
            "✓ Extracted sequences for 6 chains\n",
            "  Chain A: 558 residues\n",
            "  Chain B: 558 residues\n",
            "  Chain C: 558 residues\n",
            "  Chain D: 558 residues\n",
            "  Chain E: 558 residues\n",
            "  Chain F: 558 residues\n",
            "✓ Parsed 14856 atoms from PDB file\n",
            "  ⚡ Assigning charges...\n",
            "✓ Assigned partial charges to 14856 atoms\n",
            "  🔍 Identifying charged residues...\n",
            "✓ Identified 474 charged residues\n",
            "  🔗 Analyzing interactions...\n",
            "  🎯 Finding binding sites...\n",
            "⚠️  No charges assigned. Running charge assignment first...\n",
            "✓ Assigned partial charges to 14856 atoms\n",
            "🔍 Scanning 52576 grid points...\n",
            "  Progress: 0/16 (0%)\n",
            "  Progress: 10/16 (62%)\n",
            "✓ Found 1667 potential binding sites\n",
            "  ✅ 5K12: 14856 atoms, 474 charged residues, 1667 binding sites, 3348 total sequence length\n",
            "\n",
            "📊 Processing 5/460: 5MM2 (Overall: 5/460)\n",
            "✓ Successfully opened compressed file: 5MM2.pdb.gz\n",
            "✓ Extracted sequences for 3 chains\n",
            "  Chain A: 416 residues\n",
            "  Chain B: 251 residues\n",
            "  Chain C: 264 residues\n",
            "✓ Parsed 6605 atoms from PDB file\n",
            "  ⚡ Assigning charges...\n",
            "✓ Assigned partial charges to 6605 atoms\n",
            "  🔍 Identifying charged residues...\n",
            "✓ Identified 170 charged residues\n",
            "  🔗 Analyzing interactions...\n",
            "  🎯 Finding binding sites...\n",
            "⚠️  No charges assigned. Running charge assignment first...\n",
            "✓ Assigned partial charges to 6605 atoms\n",
            "🔍 Scanning 46368 grid points...\n",
            "  Progress: 0/23 (0%)\n",
            "  Progress: 10/23 (43%)\n",
            "  Progress: 20/23 (87%)\n",
            "✓ Found 1104 potential binding sites\n",
            "  ✅ 5MM2: 6605 atoms, 170 charged residues, 1104 binding sites, 931 total sequence length\n",
            "\n",
            "📊 Processing 6/460: 6GSH (Overall: 6/460)\n",
            "✓ Successfully opened compressed file: 6GSH.pdb.gz\n",
            "✓ Extracted sequences for 3 chains\n",
            "  Chain A: 669 residues\n",
            "  Chain B: 669 residues\n",
            "  Chain C: 669 residues\n",
            "✓ Parsed 24297 atoms from PDB file\n",
            "  ⚡ Assigning charges...\n",
            "✓ Assigned partial charges to 24297 atoms\n",
            "  🔍 Identifying charged residues...\n",
            "✓ Identified 314 charged residues\n",
            "  🔗 Analyzing interactions...\n",
            "  🎯 Finding binding sites...\n",
            "⚠️  No charges assigned. Running charge assignment first...\n",
            "✓ Assigned partial charges to 24297 atoms\n",
            "🔍 Scanning 62400 grid points...\n",
            "  Progress: 0/20 (0%)\n",
            "  Progress: 10/20 (50%)\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \"\"\"Enhanced main function with real-time analysis, sequence extraction, and resume capability.\"\"\"\n",
        "    analyzer = EnhancedChargeAffinityAnalyzer()\n",
        "\n",
        "    print(\"⚡ Enhanced Molecular Charge Affinity Analyzer v2.1\")\n",
        "    print(\"=\" * 65)\n",
        "    print(\"Features:\")\n",
        "    print(\"✓ Real-time data capture during analysis\")\n",
        "    print(\"✓ Protein sequence extraction and analysis\")\n",
        "    print(\"✓ Enhanced reporting with sequence information\")\n",
        "    print(\"✓ Progress tracking and error handling\")\n",
        "    print(\"✓ RESUME capability - picks up where it left off!\")\n",
        "    print()\n",
        "    print(\"Options:\")\n",
        "    print(\"1. Analyze single file (enhanced)\")\n",
        "    print(\"2. Batch analyze folder (real-time with resume)\")\n",
        "    print(\"3. Exit\")\n",
        "\n",
        "    mode_choice = input(\"\\nSelect mode (1-3): \").strip()\n",
        "\n",
        "    if mode_choice == '2':\n",
        "        # Enhanced batch analysis mode with resume\n",
        "        folder_path = input(\"Enter folder path containing .pdb/.pdb.gz files: \").strip()\n",
        "        if folder_path and os.path.exists(folder_path):\n",
        "            output_path = input(\"Enter output folder (press Enter for default): \").strip()\n",
        "            output_path = output_path if output_path else None\n",
        "\n",
        "            resume_choice = input(\"Resume from previous run if possible? (Y/n): \").strip().lower()\n",
        "            resume = resume_choice != 'n'\n",
        "\n",
        "            if resume:\n",
        "                print(\"\\n🔄 Starting enhanced batch analysis with RESUME capability...\")\n",
        "            else:\n",
        "                print(\"\\n🆕 Starting fresh batch analysis...\")\n",
        "\n",
        "            batch_summary = analyzer.batch_analyze_folder_realtime_with_resume(\n",
        "                folder_path, output_path, resume=resume\n",
        "            )\n",
        "\n",
        "            if batch_summary:\n",
        "                print(f\"\\n📈 Enhanced Batch Analysis Summary:\")\n",
        "                print(f\"  ✅ Total analyzed: {batch_summary.get('analyzed', 0)} proteins\")\n",
        "                print(f\"  ✗ Total failed: {batch_summary.get('failed', 0)} proteins\")\n",
        "                if 'new_analyzed' in batch_summary:\n",
        "                    print(f\"  🆕 Newly analyzed: {batch_summary['new_analyzed']} proteins\")\n",
        "                    print(f\"  ⏭️  Skipped (completed): {batch_summary.get('skipped_completed', 0)} proteins\")\n",
        "                print(f\"  📁 Results saved to: {batch_summary['output_folder']}\")\n",
        "\n",
        "                if 'realtime_files' in batch_summary:\n",
        "                    print(f\"\\n📊 Real-time files generated:\")\n",
        "                    for file_type, filename in batch_summary['realtime_files'].items():\n",
        "                        print(f\"  📄 {file_type.title()}: {filename}\")\n",
        "\n",
        "                # Quick stats\n",
        "                if batch_summary.get('results'):\n",
        "                    results = batch_summary['results']\n",
        "                    if results:\n",
        "                        avg_charge = np.mean([r['net_protein_charge'] for r in results])\n",
        "                        total_sites = sum([r['total_binding_sites'] for r in results])\n",
        "                        total_sequence_length = sum([r['sequence_length'] for r in results])\n",
        "\n",
        "                        print(f\"\\n📊 Quick Statistics (new analyses):\")\n",
        "                        print(f\"  Average net charge: {avg_charge:.2f}\")\n",
        "                        print(f\"  Total binding sites found: {total_sites}\")\n",
        "                        print(f\"  Total sequence length analyzed: {total_sequence_length} residues\")\n",
        "                        print(f\"  Proteins with strong binding sites: {len([r for r in results if r['strong_binding_sites'] > 0])}\")\n",
        "                        if len(results) > 0:\n",
        "                            print(f\"  Average sequence length: {total_sequence_length/len(results):.1f} residues/protein\")\n",
        "        else:\n",
        "            print(\"✗ Invalid folder path\")\n",
        "        return\n",
        "\n",
        "    elif mode_choice == '3':\n",
        "        print(\"👋 Goodbye!\")\n",
        "        return\n",
        "\n",
        "    elif mode_choice != '1':\n",
        "        print(\"✗ Invalid choice. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Enhanced single file analysis mode\n",
        "    filepath = input(\"Enter the path to your .pdb.gz or .pdb file: \").strip()\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"✗ File not found: {filepath}\")\n",
        "        return\n",
        "\n",
        "    # Load structure\n",
        "    content = analyzer.open_file(filepath)\n",
        "    if not content:\n",
        "        return\n",
        "\n",
        "    # Parse structure\n",
        "    if filepath.lower().endswith(('.pdb.gz', '.pdb')):\n",
        "        analyzer.parse_pdb(content)\n",
        "    else:\n",
        "        print(\"✗ Currently only PDB format supported for charge analysis\")\n",
        "        return\n",
        "\n",
        "    # Perform initial analysis\n",
        "    print(\"\\n🔬 Performing enhanced charge analysis...\")\n",
        "    analyzer.assign_partial_charges()\n",
        "    analyzer.identify_charged_residues()\n",
        "\n",
        "    # Interactive menu for single file\n",
        "    while True:\n",
        "        print(f\"\\n⚡ Enhanced Analysis Options for {analyzer.filename}:\")\n",
        "        print(\"1. Show enhanced charge analysis summary\")\n",
        "        print(\"2. Show sequence information\")\n",
        "        print(\"3. Visualize charge distribution (enhanced)\")\n",
        "        print(\"4. Find binding pockets (slow)\")\n",
        "        print(\"5. Analyze charge interactions\")\n",
        "        print(\"6. Export current analysis to CSV\")\n",
        "        print(\"7. Batch analyze folder with resume (switch to batch mode)\")\n",
        "        print(\"8. Exit\")\n",
        "\n",
        "        choice = input(\"\\nEnter your choice (1-8): \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            analyzer.print_enhanced_analysis()\n",
        "        elif choice == '2':\n",
        "            print(f\"\\n🧬 Sequence Information for {analyzer.filename}:\")\n",
        "            print(\"=\" * 50)\n",
        "            if analyzer.sequence_data:\n",
        "                total_length = 0\n",
        "                for chain_id, seq_data in analyzer.sequence_data.items():\n",
        "                    print(f\"\\nChain {chain_id}:\")\n",
        "                    print(f\"  Length: {seq_data['length']} residues\")\n",
        "                    print(f\"  SEQRES sequence: {seq_data['seqres'][:60]}{'...' if len(seq_data['seqres']) > 60 else ''}\")\n",
        "                    print(f\"  Atom-based sequence: {seq_data['atom_based'][:60]}{'...' if len(seq_data['atom_based']) > 60 else ''}\")\n",
        "                    print(f\"  Final sequence: {seq_data['final_sequence'][:60]}{'...' if len(seq_data['final_sequence']) > 60 else ''}\")\n",
        "                    total_length += seq_data['length']\n",
        "                print(f\"\\nTotal sequence length: {total_length} residues\")\n",
        "            else:\n",
        "                print(\"No sequence data available.\")\n",
        "        elif choice == '3':\n",
        "            analyzer.visualize_charge_distribution()\n",
        "        elif choice == '4':\n",
        "            print(\"🔍 Finding binding pockets (this may take a few minutes)...\")\n",
        "            analyzer.find_binding_pockets()\n",
        "            analyzer.print_enhanced_analysis()\n",
        "        elif choice == '5':\n",
        "            interactions = analyzer.analyze_charge_interactions()\n",
        "            print(f\"\\n🔗 Found {len(interactions['interactions'])} interactions\")\n",
        "            for i, interaction in enumerate(interactions['interactions'][:10]):\n",
        "                print(f\"{i+1}. {interaction['residue1']} ↔ {interaction['residue2']}: \"\n",
        "                      f\"{interaction['distance']:.1f}Å ({interaction['type']}, {interaction['strength']})\")\n",
        "        elif choice == '6':\n",
        "            # Export current analysis\n",
        "            output_dir = input(\"Enter output directory (press Enter for current directory): \").strip()\n",
        "            if not output_dir:\n",
        "                output_dir = \".\"\n",
        "\n",
        "            try:\n",
        "                os.makedirs(output_dir, exist_ok=True)\n",
        "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                protein_id = analyzer.filename.split('.')[0].upper()\n",
        "\n",
        "                # Setup writers\n",
        "                analyzer.setup_realtime_writers(output_dir, timestamp)\n",
        "\n",
        "                # Export data\n",
        "                analyzer.write_sequence_data(protein_id)\n",
        "\n",
        "                if analyzer.binding_sites:\n",
        "                    analyzer.write_binding_sites_data(protein_id)\n",
        "\n",
        "                interactions = analyzer.analyze_charge_interactions()\n",
        "                analyzer.write_interactions_data(protein_id, interactions)\n",
        "\n",
        "                protein_summary = analyzer.compile_protein_summary(protein_id, interactions)\n",
        "                analyzer.write_summary_data(protein_summary)\n",
        "\n",
        "                analyzer.close_realtime_writers()\n",
        "\n",
        "                print(f\"✓ Analysis exported to {output_dir}\")\n",
        "                print(f\"  Files created with timestamp: {timestamp}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Export failed: {e}\")\n",
        "\n",
        "        elif choice == '7':\n",
        "            folder_path = input(\"Enter folder path containing .pdb/.pdb.gz files: \").strip()\n",
        "            if folder_path and os.path.exists(folder_path):\n",
        "                output_path = input(\"Enter output folder (press Enter for default): \").strip()\n",
        "                output_path = output_path if output_path else None\n",
        "\n",
        "                resume_choice = input(\"Resume from previous run if possible? (Y/n): \").strip().lower()\n",
        "                resume = resume_choice != 'n'\n",
        "\n",
        "                if resume:\n",
        "                    print(\"\\n🔄 Starting enhanced batch analysis with RESUME capability...\")\n",
        "                else:\n",
        "                    print(\"\\n🆕 Starting fresh batch analysis...\")\n",
        "\n",
        "                batch_summary = analyzer.batch_analyze_folder_realtime_with_resume(\n",
        "                    folder_path, output_path, resume=resume\n",
        "                )\n",
        "\n",
        "                if batch_summary:\n",
        "                    print(f\"\\n📈 Enhanced Batch Analysis Summary:\")\n",
        "                    print(f\"  ✅ Total analyzed: {batch_summary.get('analyzed', 0)} proteins\")\n",
        "                    print(f\"  ✗ Total failed: {batch_summary.get('failed', 0)} proteins\")\n",
        "                    if 'new_analyzed' in batch_summary:\n",
        "                        print(f\"  🆕 Newly analyzed: {batch_summary['new_analyzed']} proteins\")\n",
        "                        print(f\"  ⏭️  Skipped (completed): {batch_summary.get('skipped_completed', 0)} proteins\")\n",
        "                    print(f\"  📁 Results saved to: {batch_summary['output_folder']}\")\n",
        "\n",
        "                    if 'realtime_files' in batch_summary:\n",
        "                        print(f\"\\n📊 Real-time files generated:\")\n",
        "                        for file_type, filename in batch_summary['realtime_files'].items():\n",
        "                            print(f\"  📄 {file_type.title()}: {filename}\")\n",
        "            else:\n",
        "                print(\"✗ Invalid folder path\")\n",
        "        elif choice == '8':\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid choice. Please enter 1-8.\")\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}